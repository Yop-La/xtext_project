
mml_data = read.csv("/home/ensai/Dropbox/ENSAI/Ressources/S5/dsl/projet/ensai-sid-mml/org.xtext.example.mml/mml_data/iris.csv", sep=",")      
set.seed(0)
options(warn=-1)
target_index = "-1"

if(is.na(strtoi(target_index))){
  target_index = grep(target_index, colnames(mml_data))
  
}else{
  if(target_index == -1){
    target_index = length(colnames(mml_data))
  }
  target_index = strtoi(target_index)
}

target = mml_data[,target_index]
predictors =mml_data[,-target_index]
require(rpart)
train_model <- function(X_train, y_train){
  model = rpart(y_train ~ ., data = X_train)
}

test_model <- function(model,X_test){
  pred = predict(model, X_test, type="class")
  return(pred)
}

metrics = "F1+RECALL+PRECISION";

metrics = strsplit(metrics, "+",fixed=TRUE)[[1]]

compute_metrics <- function(predicted, expected,metrics) {
  predicted <- factor(as.character(predicted), levels=unique(as.character(expected)))
  expected  <- as.factor(expected)
  cm = as.matrix(table(expected, predicted))
  
  precision <- diag(cm) / colSums(cm)
  recall <- diag(cm) / rowSums(cm)
  f1 <-  ifelse(precision + recall == 0, 0, 2 * precision * recall / (precision + recall))
  
  #Assuming that F1 is zero when it's not possible compute it
  f1[is.na(f1)] <- 0
  precision[is.na(precision)] <- 0
  recall[is.na(recall)] <- 0
  
  
  #Binary F1 or Multi-class macro-averaged F1
  f1 <-  ifelse(nlevels(expected) == 2, f1[positive.class], mean(f1))
  precision <-  ifelse(nlevels(expected) == 2, precision[positive.class], mean(precision))
  recall <-  ifelse(nlevels(expected) == 2, recall[positive.class], mean(recall))
  
  ret <- list(F1 = f1,PRECISION = precision,RECALL = recall)
  
  return(ret[metrics])
}

n_splits = 5

require(caret)
folds_index = createFolds(predictors$X, k = n_splits, list = TRUE, returnTrain = TRUE)
f1 = c();
precision = c();
recall = c();
for(train_ind in folds_index){
  
  X_train = predictors[train_ind,]
  X_test = predictors[-train_ind,]
  y_train = target[train_ind]
  y_test = target[-train_ind]
  
  model = train_model(X_train, y_train)
  
  y_prediction = test_model(model, X_test)
  
  metrics_res = compute_metrics(y_prediction,y_test,metrics)
  
  if(!is.null(metrics_res$RECALL)){
    recall = c(recall,metrics_res$RECALL)
  }
  
  if(!is.null(metrics_res$PRECISION)){
    precision = c(precision,metrics_res$PRECISION)
  }
  if(!is.null(metrics_res$F1)){
    f1 = c(f1,metrics_res$F1)
  }
  
  
}

metrics_final=list();
if(!is.null(f1)){
  metrics_final$F1 = mean(f1)
}
if(!is.null(precision)){
  metrics_final$PRECISION = mean(precision)
}
if(!is.null(recall)){
  metrics_final$RECALL = mean(recall)
}

metrics = metrics_final

require(rjson)
cat(toJSON(metrics))


